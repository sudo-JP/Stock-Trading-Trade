# Threading System Design 

## Overview
The threading system follows a producer–consumer synchronization model.  
Producers are responsible for assigning tasks into a shared work queue, while consumers retrieve and execute these tasks concurrently.

The work queue uses a dynamically growing structure rather than a fixed-size buffer, since the volume of work generated by producers (in this case, signal updates from ML models through the Rust backend) is unpredictable.  

Each consumer thread fetches multiple tasks at once, controlled by a constant `CHUNK` value. This batching reduces lock contention and improves throughput compared to one-at-a-time retrieval.

One specialized thread separately manages account and position synchronization, as these require serialized access to shared state. All other tasks are designed to be stateless, making them suitable for distribution across the consumer pool.

### Thread Roles
- **Producers:** Receive ML model signals via Rust TCP interface and enqueue closures for execution.
- **Consumers:** Pop and execute batched tasks from the queue.
- **Account Thread:** Periodically syncs account and position data, updating shared cache.

## Components 

### TaskPool

**Purpose:**  
A dynamic work queue for producers to enqueue tasks and consumers to execute them concurrently.

**Data Members:**  
- `deque<std::function<void()>> works` — stores closures representing tasks.  
- `std::mutex mtx` — protects concurrent access to the queue.  
- `std::condition_variable cv` — allows consumers to wait when the queue is empty.

**Synchronization:**  
- Consumers block on the CV if the queue is empty.  
- Producers never block while inserting, since the deque dynamically grows.  
- After adding tasks, producers signal all sleeping consumers to wake up.  
  This ensures that multiple consumers waiting on the CV can resume execution, preventing deadlocks.

**Methods:**  
- `add_work()` — thread-safe method to enqueue closures. Signals consumers after insertion.  
- `get_work()` — retrieves up to `CHUNK` closures at once for batch execution by a consumer.

### Thread Lifecycle

All producer and consumer threads follow this loop:

1. **Thread Creation / Initialization**
    - Perform handshake with Rust backend and Python ML processes to synchronize thread/process counts and TCP port allocations.
    - Thread is spawned and registered as either a producer or a consumer.
    - Each thread associates with the shared `system_down` variable for graceful shutdown.

2. **Wait for System Ready**
    - **Consumers:** Block on a condition variable if the work queue is empty.  
    - **Producers:** Wait for handshake to complete before starting to receive work.

3. **Main Execution Loop**
    - **Consumers:** While `system_down` is false:
        1. Check the task queue (`TaskPool`) for available work.
        2. If the queue is empty, block on the CV until a producer signals new tasks.
        3. Pop up to `CHUNK` closures from the queue.
        4. Execute each closure in sequence. Each closure contains all context needed (service instance, symbol, parameters).
        5. Handle exceptions locally to prevent thread crashes.
        6. Repeat the loop.
    - **Producers:** While `system_down` is false:
        1. Receive data via the TCP protocol.
        2. Create closures from the service modules with the necessary arguments.
        3. Push the vector of closures into the task queue (`TaskPool`).
        4. Repeat the loop.

4. **System Shutdown**
    - When `system_down` is true, threads exit their main loop.
    - Perform any necessary cleanup (release resources, notify thread manager).
